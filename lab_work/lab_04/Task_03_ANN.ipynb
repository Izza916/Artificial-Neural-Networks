{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Iris Classification"
      ],
      "metadata": {
        "id": "bIVb1qGWQI3H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhozxWjGP9QT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "y = encoder.fit_transform(y).toarray()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros(hidden_size)\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros(output_size)\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exps = np.exp(x - np.max(x))\n",
        "        return exps / exps.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.relu(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.softmax(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def compute_loss(self, y_true):\n",
        "        m = y_true.shape[0]\n",
        "        return -np.sum(y_true * np.log(self.a2 + 1e-15)) / m\n",
        "\n",
        "    def backward(self, X, y_true, lr):\n",
        "        m = y_true.shape[0]\n",
        "\n",
        "        dz2 = self.a2 - y_true\n",
        "        dw2 = np.dot(self.a1.T, dz2) / m\n",
        "        db2 = np.sum(dz2, axis=0) / m\n",
        "\n",
        "        dz1 = np.dot(dz2, self.W2.T) * (self.z1 > 0)\n",
        "        dw1 = np.dot(X.T, dz1) / m\n",
        "        db1 = np.sum(dz1, axis=0) / m\n",
        "\n",
        "        self.W2 -= lr * dw2\n",
        "        self.b2 -= lr * db2\n",
        "        self.W1 -= lr * dw1\n",
        "        self.b1 -= lr * db1\n",
        "\n",
        "    def train(self, X, y, epochs=1000, lr=0.01):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            loss = self.compute_loss(y)\n",
        "            self.backward(X, y, lr)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.forward(X), axis=1)"
      ],
      "metadata": {
        "id": "9SzUJk7aQLzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork(input_size=4, hidden_size=10, output_size=3)\n",
        "\n",
        "nn.train(X_train, y_train, epochs=1000, lr=0.01)\n",
        "\n",
        "y_pred = nn.predict(X_test)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "accuracy = np.mean(y_pred == y_true)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDXaGveQQPpd",
        "outputId": "21eda384-9523-45a3-e2a9-53394927d576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.0987\n",
            "Epoch 100, Loss: 1.0979\n",
            "Epoch 200, Loss: 1.0959\n",
            "Epoch 300, Loss: 1.0894\n",
            "Epoch 400, Loss: 1.0669\n",
            "Epoch 500, Loss: 0.9997\n",
            "Epoch 600, Loss: 0.8700\n",
            "Epoch 700, Loss: 0.7443\n",
            "Epoch 800, Loss: 0.6542\n",
            "Epoch 900, Loss: 0.5834\n",
            "Test Accuracy: 0.8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: News Classification"
      ],
      "metadata": {
        "id": "5Dc99iWsQTLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "categories = ['talk.politics.misc', 'rec.sport.baseball', 'misc.forsale', 'sci.space']\n",
        "\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "X_text = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(X_text).toarray()\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Zy5o4NmtQWMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassifier(NeuralNetwork):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__(input_size, hidden_size, output_size)\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "nn_text = TextClassifier(input_size=input_size, hidden_size=64, output_size=4)\n",
        "\n",
        "nn_text.train(X_train, y_train, epochs=500, lr=0.01)\n",
        "\n",
        "y_pred = nn_text.predict(X_test)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "accuracy = np.mean(y_pred == y_true)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L18vkp8AQ1Is",
        "outputId": "8f8d716f-529b-4b66-b420-27064a267a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.3851\n",
            "Epoch 100, Loss: 1.3517\n",
            "Epoch 200, Loss: 1.3203\n",
            "Epoch 300, Loss: 1.2818\n",
            "Epoch 400, Loss: 1.2282\n",
            "Test Accuracy: 0.6091\n"
          ]
        }
      ]
    }
  ]
}